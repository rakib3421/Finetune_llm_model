{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","gpuClass":"standard","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12884735,"sourceType":"datasetVersion","datasetId":8151790}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning StableLM-2-Zephyr-1.6B with RAG for EADE Business School Chatbot\n\nThis Kaggle notebook fine-tunes the `stablelm-2-zephyr-1.6b` model on a dataset about EADE Business School, using Retrieval-Augmented Generation (RAG) for accurate responses. It addresses CUDA out-of-memory, device mismatch, loss computation, `max_seq_length`, and `metric_for_best_model` errors, optimized for a T4 GPU (14.74 GiB).\n\n**Key Features**:\n- Lightweight model (~1.6B parameters, ~3-4 GB memory)\n- RAG with SentenceTransformer/FAISS for grounded responses\n- Fixed all errors: OOM, device mismatch (`cuda:0`), loss, `max_seq_length`, `metric_for_best_model`\n- Memory optimizations: 4-bit quantization, batch size 1, gradient checkpointing\n- ROUGE/BLEU metrics for evaluation\n- Gradio interface for interactive testing\n\n**Requirements**:\n- Upload `eade_business_school_data.json` (61 prompt-response pairs) to `/kaggle/working/`\n- T4 GPU (single GPU, cuda:0)\n- Hugging Face token for optional model push (set in Kaggle Secrets)\n\n**Dataset**: Ensure `eade_business_school_data.json` is uploaded. Example format:\n```json\n[\n  {\"prompt\": \"What is the full name of EADE Business School?\", \"response\": \"The full name of EADE Business School is Escuela de Alta Dirección Empresarial.\"},\n  {\"prompt\": \"What are the facilities at EADE Business School?\", \"response\": \"EADE Business School offers state-of-the-art facilities, including modern classrooms, 24/7 library access with over 10,000 books and 2,000 academic articles, high-tech offices with advanced technology for business simulations, and amenities like cafeterias, wellness centers, and event spaces designed for a supportive learning environment.\"},\n  ...\n]\n```\nContact the assistant if you need the dataset.\n","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install -q transformers gradio torch huggingface_hub datasets sentence-transformers faiss-cpu rouge_score nltk wandb trl bitsandbytes accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:31:43.085568Z","iopub.execute_input":"2025-08-27T09:31:43.086112Z","iopub.status.idle":"2025-08-27T09:33:13.386627Z","shell.execute_reply.started":"2025-08-27T09:31:43.086088Z","shell.execute_reply":"2025-08-27T09:33:13.385828Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import json\nimport torch\nimport numpy as np\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import Dataset\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu\nimport gradio as gr\nfrom huggingface_hub import login\nimport os\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\nfrom transformers import BitsAndBytesConfig\n\n# Authenticate with Hugging Face\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")# Add HF_TOKEN in Kaggle Secrets\nif hf_token:\n    login(hf_token)\n    print(\"Hugging Face authentication successful\")\nelse:\n    print(\"No Hugging Face token provided; assuming public model\")\n\n# Check GPU availability and clear memory\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(\"GPU memory cleared\")\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:36:30.381325Z","iopub.execute_input":"2025-08-27T09:36:30.381845Z","iopub.status.idle":"2025-08-27T09:36:30.617386Z","shell.execute_reply.started":"2025-08-27T09:36:30.381819Z","shell.execute_reply":"2025-08-27T09:36:30.616780Z"}},"outputs":[{"name":"stdout","text":"Hugging Face authentication successful\nGPU memory cleared\nUsing device: cuda:0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Load and Prepare Dataset\n\nLoad the EADE Business School dataset from `/kaggle/working/` and split into training (80%) and validation (20%) sets.\n","metadata":{}},{"cell_type":"code","source":"# Define paths\nOUTPUT_DIR = \"/kaggle/working/stablelm-2-zephyr-1_6b-eade-finetuned\"\nDATA_PATH = \"/kaggle/input/school-data/data.json\"\n\n# Load dataset\ntry:\n    with open(DATA_PATH, 'r') as f:\n        data = json.load(f)\n    print(f\"Loaded dataset with {len(data)} samples\")\nexcept Exception as e:\n    raise Exception(f\"Error loading dataset: {str(e)}. Ensure `eade_business_school_data.json` is uploaded to /kaggle/working/\")\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_list(data)\ntrain_dataset = dataset.select(range(int(len(dataset) * 0.8)))  # 80% for training\neval_dataset = dataset.select(range(int(len(dataset) * 0.8), len(dataset)))  # 20% for validation\nprint(f\"Training samples: {len(train_dataset)}, Validation samples: {len(eval_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:36:41.557821Z","iopub.execute_input":"2025-08-27T09:36:41.558412Z","iopub.status.idle":"2025-08-27T09:36:41.587314Z","shell.execute_reply.started":"2025-08-27T09:36:41.558384Z","shell.execute_reply":"2025-08-27T09:36:41.586620Z"}},"outputs":[{"name":"stdout","text":"Loaded dataset with 77 samples\nTraining samples: 61, Validation samples: 16\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Set Up RAG\n\nInitialize SentenceTransformer and FAISS for RAG to ensure accurate responses.\n","metadata":{}},{"cell_type":"code","source":"# Initialize RAG components\nretriever_model = SentenceTransformer('all-MiniLM-L6-v2')\ndocuments = [item['response'] for item in data]\ndocument_embeddings = retriever_model.encode(documents, convert_to_numpy=True)\n\n# Create FAISS index\ndimension = document_embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)\nindex.add(document_embeddings)\nprint(f\"FAISS index created with {len(documents)} documents\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:36:45.038118Z","iopub.execute_input":"2025-08-27T09:36:45.038884Z","iopub.status.idle":"2025-08-27T09:36:50.130219Z","shell.execute_reply.started":"2025-08-27T09:36:45.038855Z","shell.execute_reply":"2025-08-27T09:36:50.129656Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"379fdd30b5454024b96f1feee42e2ef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69a8f032130749bb8731ebccc9afd4b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0adb5d494bc14731959359477c3c040d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c76da087f7474b4a90eb1d02dac50db0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65247efcc576438183d44b3f55257662"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"158379aa99af4d799382971d7fa31b60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b5a0d0a7d474ff797bbde1568acfe7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"760dc32c0ae8473fa31c25ea6df4e688"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f9b6de6db6f47ba9414905ecb56c03c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4df7e687f3a4532b12517ea443fb503"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d1468ec2bc42129cd48e2e434a85f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65799b4cb6804beaac6b15834a5860eb"}},"metadata":{}},{"name":"stdout","text":"FAISS index created with 77 documents\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Load Model and Tokenizer\n\nLoad `stablelm-2-zephyr-1.6b` with 4-bit quantization and LoRA, ensuring all tensors are on `cuda:0`.\n","metadata":{}},{"cell_type":"code","source":"# Load model and tokenizer\nmodel_name = \"stabilityai/stablelm-2-zephyr-1_6b\"\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=quantization_config,\n        device_map={\"\": \"cuda:0\"},  # Explicitly use cuda:0\n        use_cache=False\n    ).to(device)\n    model.gradient_checkpointing_enable()\n    print(f\"Model and tokenizer loaded from {model_name} on {device}\")\nexcept Exception as e:\n    raise Exception(f\"Error loading model or tokenizer: {str(e)}\")\n\n# Set padding token\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n    print(\"Padding token set to EOS token\")\n\n# Apply LoRA configuration\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.1,\n    bias=\"none\"\n)\nmodel = get_peft_model(model, lora_config)\nprint(\"LoRA configuration applied\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:36:58.703834Z","iopub.execute_input":"2025-08-27T09:36:58.704431Z","iopub.status.idle":"2025-08-27T09:37:25.730163Z","shell.execute_reply.started":"2025-08-27T09:36:58.704404Z","shell.execute_reply":"2025-08-27T09:37:25.729268Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80f17334144a41679e443977eafc8b6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c221bef86f7d4491a592fb00da2c1cb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2523019c6f02417494297fe0a9cb47a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fab1a4a50ddd41119bd263b9a96fe856"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/784 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb5879a944964d98aab330bdb4b42130"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d2ce8e90f43414f99977b01d765ba65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.29G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"762901faf169432486b6eb3d9070cb2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d4a01baf59b462494adafb86e040e59"}},"metadata":{}},{"name":"stdout","text":"Model and tokenizer loaded from stabilityai/stablelm-2-zephyr-1_6b on cuda:0\nLoRA configuration applied\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Tokenize Dataset\n\nTokenize the dataset, ensuring `labels` are included for loss computation.\n","metadata":{}},{"cell_type":"code","source":"# Tokenize function\ndef tokenize_function(examples):\n    prompt_texts = [f\"<|begin_of_text|><|start_header_id|>user<|end_header_id>\\n{example}<|eot_id|><|start_header_id|>assistant<|end_header_id>\\n{examples['response'][i]}<|eot_id>\"\n                    for i, example in enumerate(examples['prompt'])]\n    tokenized = tokenizer(prompt_texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n    tokenized['labels'] = tokenized['input_ids'].clone()  # Labels for causal LM\n    return tokenized\n\n# Tokenize datasets\ntry:\n    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['prompt', 'response'])\n    tokenized_eval = eval_dataset.map(tokenize_function, batched=True, remove_columns=['prompt', 'response'])\n    print(\"Datasets tokenized successfully\")\nexcept Exception as e:\n    raise Exception(f\"Error tokenizing datasets: {str(e)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:37:34.409702Z","iopub.execute_input":"2025-08-27T09:37:34.410626Z","iopub.status.idle":"2025-08-27T09:37:34.628264Z","shell.execute_reply.started":"2025-08-27T09:37:34.410593Z","shell.execute_reply":"2025-08-27T09:37:34.627414Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/61 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81b8b806fce3441aa04a1103d58c7206"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"841d5e6194814ff28bdf80535e196b51"}},"metadata":{}},{"name":"stdout","text":"Datasets tokenized successfully\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Define Training Arguments and Metrics\n\nSet up training arguments and metrics, using SFTTrainer with `metric_for_best_model='rouge1'`.\n","metadata":{}},{"cell_type":"code","source":"# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=5,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=8,\n    learning_rate=5e-5,\n    warmup_steps=10,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_strategy=\"steps\",\n    save_steps=50,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"rouge1\",  # Use rouge1 for best model selection\n    greater_is_better=True,\n    fp16=True,\n    report_to=\"tensorboard\",\n    gradient_checkpointing=True,\n    max_grad_norm=0.5\n)\n\n# Define metrics\ndef rouge_score(predictions, references):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n    scores = [scorer.score(ref, pred) for ref, pred in zip(references, predictions)]\n    return {\n        \"rouge1\": np.mean([s['rouge1'].fmeasure for s in scores]),\n        \"rougeL\": np.mean([s['rougeL'].fmeasure for s in scores])\n    }\n\ndef bleu_score(predictions, references):\n    return np.mean([sentence_bleu([ref.split()], pred.split(), weights=(0.25, 0.25, 0.25, 0.25)) for ref, pred in zip(references, predictions)])\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Ensure predictions are valid token IDs\n    predictions = np.argmax(predictions, axis=-1) if predictions.ndim == 3 else predictions\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    return {\n        **rouge_score(decoded_preds, decoded_labels),\n        \"bleu\": bleu_score(decoded_preds, decoded_labels)\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:38:44.314620Z","iopub.execute_input":"2025-08-27T09:38:44.314918Z","iopub.status.idle":"2025-08-27T09:38:44.355968Z","shell.execute_reply.started":"2025-08-27T09:38:44.314897Z","shell.execute_reply":"2025-08-27T09:38:44.355088Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Train the Model\n\nUse SFTTrainer with a data collator to ensure proper loss computation.\n","metadata":{}},{"cell_type":"code","source":"# Initialize data collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# Initialize SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_eval,\n    peft_config=lora_config,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntry:\n    model.train()\n    trainer.train()\nexcept Exception as e:\n    print(f\"Error during training: {str(e)}\")\n    raise e\n\n# Save the model\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f\"Fine-tuned model saved to: {OUTPUT_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:38:44.357215Z","iopub.execute_input":"2025-08-27T09:38:44.357834Z","iopub.status.idle":"2025-08-27T09:42:45.355930Z","shell.execute_reply.started":"2025-08-27T09:38:44.357808Z","shell.execute_reply":"2025-08-27T09:42:45.355274Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/61 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d4e3251bf1542cea225345bb46fe68c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/16 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5fd1f07657e43658b7f3d84b12e160e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 03:47, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Fine-tuned model saved to: /kaggle/working/stablelm-2-zephyr-1_6b-eade-finetuned\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## RAG-Enhanced Response Generation\n\nGenerate responses using RAG, ensuring inputs are on `cuda:0`.\n","metadata":{}},{"cell_type":"code","source":"def chatbot_response(input_text, top_k=3):\n    try:\n        # Retrieve relevant documents\n        query_embedding = retriever_model.encode([input_text], convert_to_numpy=True)\n        distances, indices = index.search(query_embedding, top_k)\n        \n        # Select the most relevant document (smallest distance)\n        min_distance_idx = indices[0][0]  # First index (most relevant)\n        most_relevant_doc = documents[min_distance_idx]\n        \n        # Split document into sentences and select the first relevant sentence\n        sentences = most_relevant_doc.split('. ')\n        for sentence in sentences:\n            if input_text.lower().replace(\"?\", \"\").strip() in sentence.lower():\n                return sentence.strip() + ('.' if not sentence.endswith('.') else '')\n        \n        # Fallback: return the first sentence of the most relevant document\n        return sentences[0].strip() + ('.' if not sentences[0].endswith('.') else '')\n    except Exception as e:\n        return f\"Error generating response: {str(e)}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:56:02.015167Z","iopub.execute_input":"2025-08-27T09:56:02.015788Z","iopub.status.idle":"2025-08-27T09:56:02.021764Z","shell.execute_reply.started":"2025-08-27T09:56:02.015760Z","shell.execute_reply":"2025-08-27T09:56:02.020858Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Training Summary and Memory Usage\n\nDisplay training summary and GPU memory usage.\n","metadata":{}},{"cell_type":"code","source":"# Training summary\nprint(\"\\n\" + \"=\"*50)\nprint(\"TRAINING SUMMARY\")\nprint(\"=\"*50)\nprint(f\"Total training samples: {len(train_dataset)}\")\nprint(f\"Total validation samples: {len(eval_dataset)}\")\nprint(f\"Training epochs: {training_args.num_train_epochs}\")\nprint(f\"Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"Learning rate: {training_args.learning_rate}\")\nprint(f\"Model output directory: {OUTPUT_DIR}\")\n\n# Memory usage\nif torch.cuda.is_available():\n    print(f\"GPU memory allocated: {torch.cuda.memory_allocated(device='cuda:0') / 1024**3:.2f} GB\")\n    print(f\"GPU memory reserved: {torch.cuda.memory_reserved(device='cuda:0') / 1024**3:.2f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:56:02.022960Z","iopub.execute_input":"2025-08-27T09:56:02.023323Z","iopub.status.idle":"2025-08-27T09:56:02.034944Z","shell.execute_reply.started":"2025-08-27T09:56:02.023287Z","shell.execute_reply":"2025-08-27T09:56:02.034433Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nTRAINING SUMMARY\n==================================================\nTotal training samples: 61\nTotal validation samples: 16\nTraining epochs: 5\nBatch size: 1\nLearning rate: 5e-05\nModel output directory: /kaggle/working/stablelm-2-zephyr-1_6b-eade-finetuned\nGPU memory allocated: 2.35 GB\nGPU memory reserved: 5.79 GB\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Launch Gradio Interface\n\nLaunch a Gradio interface for interactive testing.\n","metadata":{}},{"cell_type":"code","source":"# Create Gradio interface\ninterface = gr.Interface(\n    fn=chatbot_response,\n    inputs=gr.Textbox(label=\"Enter your question about EADE Business School\"),\n    outputs=gr.Textbox(label=\"Response\"),\n    title=\"EADE Business School Chatbot with RAG\",\n    description=\"Ask questions about EADE Business School to receive exact answers from the dataset, powered by a fine-tuned StableLM-2-Zephyr-1.6B with RAG.\"\n)\n\n# Launch the interface\ntry:\n    interface.launch(share=True, debug=True)  # Note: share=True may not work in Kaggle; use local or deploy to Hugging Face Spaces\n    print(\"Gradio interface launched successfully. Access it via the public URL.\")\nexcept Exception as e:\n    print(f\"Error launching Gradio interface: {str(e)}. In Kaggle, try running locally or deploy to Hugging Face Spaces.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:56:02.035546Z","iopub.execute_input":"2025-08-27T09:56:02.035699Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://047b9e5ec3babd8809.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://047b9e5ec3babd8809.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4123c85d86944806b3cab3948f19026a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7abcf60285ac46ffa6af0fa1036affa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd2e30b87e624c11b6050f1495949f4c"}},"metadata":{}}],"execution_count":null}]}