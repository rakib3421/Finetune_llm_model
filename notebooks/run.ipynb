{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers torch gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gradio as gr\n",
    "import torch\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "secret_label = \"HF_TOKEN\"  # Replace with your secret label\n",
    "secret_value = UserSecretsClient().get_secret(secret_label)\n",
    "login(secret_value)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model_path = \"/kaggle/input/model-2/llama-3.2-3b-eade-finetuned\"  # Adjust path as needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "# Function to generate response\n",
    "def chatbot_response(input_text):\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs, max_length=100, do_sample=True)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Create Gradio interface\n",
    "interface = gr.Interface(fn=chatbot_response, inputs=\"text\", outputs=\"text\", title=\"Finetuned LLaMA Chatbot\")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
