{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning StableLM-2-Zephyr-1.6B with RAG for EADE Business School Chatbot\n",
        "\n",
        "This Kaggle notebook fine-tunes the `stablelm-2-zephyr-1.6b` model on a dataset about EADE Business School, using Retrieval-Augmented Generation (RAG) for accurate responses. It addresses CUDA out-of-memory, device mismatch, loss computation, `max_seq_length`, and `metric_for_best_model` errors, optimized for a T4 GPU (14.74 GiB).\n",
        "\n",
        "**Key Features**:\n",
        "- Lightweight model (~1.6B parameters, ~3-4 GB memory)\n",
        "- RAG with SentenceTransformer/FAISS for grounded responses\n",
        "- Fixed all errors: OOM, device mismatch (`cuda:0`), loss, `max_seq_length`, `metric_for_best_model`\n",
        "- Memory optimizations: 4-bit quantization, batch size 1, gradient checkpointing\n",
        "- ROUGE/BLEU metrics for evaluation\n",
        "- Gradio interface for interactive testing\n",
        "\n",
        "**Requirements**:\n",
        "- Upload `eade_business_school_data.json` (61 prompt-response pairs) to `/kaggle/working/`\n",
        "- T4 GPU (single GPU, cuda:0)\n",
        "- Hugging Face token for optional model push (set in Kaggle Secrets)\n",
        "\n",
        "**Dataset**: Ensure `eade_business_school_data.json` is uploaded. Example format:\n",
        "```json\n",
        "[\n",
        "  {\"prompt\": \"What is the full name of EADE Business School?\", \"response\": \"The full name of EADE Business School is Escuela de Alta Direcci√≥n Empresarial.\"},\n",
        "  {\"prompt\": \"What are the facilities at EADE Business School?\", \"response\": \"EADE Business School offers state-of-the-art facilities, including modern classrooms, 24/7 library access with over 10,000 books and 2,000 academic articles, high-tech offices with advanced technology for business simulations, and amenities like cafeterias, wellness centers, and event spaces designed for a supportive learning environment.\"},\n",
        "  ...\n",
        "]\n",
        "```\n",
        "Contact the assistant if you need the dataset.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers gradio torch huggingface_hub datasets sentence-transformers faiss-cpu rouge_score nltk wandb trl bitsandbytes accelerate\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import gradio as gr\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "from transformers import BitsAndBytesConfig\n",
        "import wandb\n",
        "\n",
        "# Restrict to single GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# Set environment variable to reduce memory fragmentation\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Initialize Weights & Biases\n",
        "wandb.init(project=\"eade-chatbot-stablelm-1.6b\", anonymous=\"allow\")\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "hf_token = user_secrets.get_secret(\"HF_TOKEN\")  # Add HF_TOKEN in Kaggle Secrets\n",
        "if hf_token:\n",
        "    login(hf_token)\n",
        "    print(\"Hugging Face authentication successful\")\n",
        "else:\n",
        "    print(\"No Hugging Face token provided; assuming public model\")\n",
        "\n",
        "# Check GPU availability and clear memory\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU memory cleared\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Prepare Dataset\n",
        "\n",
        "Load the EADE Business School dataset from `/kaggle/working/` and split into training (80%) and validation (20%) sets.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "OUTPUT_DIR = \"/kaggle/working/stablelm-2-zephyr-1_6b-eade-finetuned\"\n",
        "DATA_PATH = \"/kaggle/working/eade_business_school_data.json\"\n",
        "\n",
        "# Load dataset\n",
        "try:\n",
        "    with open(DATA_PATH, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"Loaded dataset with {len(data)} samples\")\n",
        "except Exception as e:\n",
        "    raise Exception(f\"Error loading dataset: {str(e)}. Ensure `eade_business_school_data.json` is uploaded to /kaggle/working/\")\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_list(data)\n",
        "train_dataset = dataset.select(range(int(len(dataset) * 0.8)))  # 80% for training\n",
        "eval_dataset = dataset.select(range(int(len(dataset) * 0.8), len(dataset)))  # 20% for validation\n",
        "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(eval_dataset)}\")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up RAG\n",
        "\n",
        "Initialize SentenceTransformer and FAISS for RAG to ensure accurate responses.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize RAG components\n",
        "retriever_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "documents = [item['response'] for item in data]\n",
        "document_embeddings = retriever_model.encode(documents, convert_to_numpy=True)\n",
        "\n",
        "# Create FAISS index\n",
        "dimension = document_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(document_embeddings)\n",
        "print(f\"FAISS index created with {len(documents)} documents\")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model and Tokenizer\n",
        "\n",
        "Load `stablelm-2-zephyr-1.6b` with 4-bit quantization and LoRA, ensuring all tensors are on `cuda:0`.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"stabilityai/stablelm-2-zephyr-1_6b\"\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map={\"\": \"cuda:0\"},  # Explicitly use cuda:0\n",
        "        use_cache=False\n",
        "    ).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    print(f\"Model and tokenizer loaded from {model_name} on {device}\")\n",
        "except Exception as e:\n",
        "    raise Exception(f\"Error loading model or tokenizer: {str(e)}\")\n",
        "\n",
        "# Set padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    print(\"Padding token set to EOS token\")\n",
        "\n",
        "# Apply LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"LoRA configuration applied\")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize Dataset\n",
        "\n",
        "Tokenize the dataset, ensuring `labels` are included for loss computation.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize function\n",
        "def tokenize_function(examples):\n",
        "    prompt_texts = [f\"<|begin_of_text|><|start_header_id|>user<|end_header_id>\\n{example}<|eot_id|><|start_header_id|>assistant<|end_header_id>\\n{examples['response'][i]}<|eot_id>\"\n",
        "                    for i, example in enumerate(examples['prompt'])]\n",
        "    tokenized = tokenizer(prompt_texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
        "    tokenized['labels'] = tokenized['input_ids'].clone()  # Labels for causal LM\n",
        "    return tokenized\n",
        "\n",
        "# Tokenize datasets\n",
        "try:\n",
        "    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['prompt', 'response'])\n",
        "    tokenized_eval = eval_dataset.map(tokenize_function, batched=True, remove_columns=['prompt', 'response'])\n",
        "    print(\"Datasets tokenized successfully\")\n",
        "except Exception as e:\n",
        "    raise Exception(f\"Error tokenizing datasets: {str(e)}\")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training Arguments and Metrics\n",
        "\n",
        "Set up training arguments and metrics, using SFTTrainer with `metric_for_best_model='rouge1'`.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=10,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rouge1\",  # Use rouge1 for best model selection\n",
        "    greater_is_better=True,\n",
        "    fp16=True,\n",
        "    report_to=\"wandb\",\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=0.5\n",
        ")\n",
        "\n",
        "# Define metrics\n",
        "def rouge_score(predictions, references):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    scores = [scorer.score(ref, pred) for ref, pred in zip(references, predictions)]\n",
        "    return {\n",
        "        \"rouge1\": np.mean([s['rouge1'].fmeasure for s in scores]),\n",
        "        \"rougeL\": np.mean([s['rougeL'].fmeasure for s in scores])\n",
        "    }\n",
        "\n",
        "def bleu_score(predictions, references):\n",
        "    return np.mean([sentence_bleu([ref.split()], pred.split(), weights=(0.25, 0.25, 0.25, 0.25)) for ref, pred in zip(references, predictions)])\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    # Ensure predictions are valid token IDs\n",
        "    predictions = np.argmax(predictions, axis=-1) if predictions.ndim == 3 else predictions\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    return {\n",
        "        **rouge_score(decoded_preds, decoded_labels),\n",
        "        \"bleu\": bleu_score(decoded_preds, decoded_labels)\n",
        "    }\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model\n",
        "\n",
        "Use SFTTrainer with a data collator to ensure proper loss computation.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Initialize SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    peft_config=lora_config,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "try:\n",
        "    model.train()\n",
        "    trainer.train()\n",
        "except Exception as e:\n",
        "    print(f\"Error during training: {str(e)}\")\n",
        "    raise e\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"Fine-tuned model saved to: {OUTPUT_DIR}\")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG-Enhanced Response Generation\n",
        "\n",
        "Generate responses using RAG, ensuring inputs are on `cuda:0`.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG-based response generation\n",
        "def chatbot_response(input_text, max_new_tokens=100, temperature=0.7, top_p=0.9, top_k=3):\n",
        "    try:\n",
        "        # Retrieve relevant documents\n",
        "        query_embedding = retriever_model.encode([input_text], convert_to_numpy=True)\n",
        "        distances, indices = index.search(query_embedding, top_k)\n",
        "        retrieved_docs = [documents[i] for i in indices[0]]\n",
        "        context = \"\\n\".join(retrieved_docs)\n",
        "\n",
        "        # Format prompt with RAG context\n",
        "        formatted_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id>\n",
        "Context: {context}\n",
        "Question: {input_text}\n",
        "Answer only with verified information from the provided context.<|eot_id|><|start_header_id|>assistant<|end_header_id>\"\"\"\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(\n",
        "            formatted_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=256\n",
        "        ).to(device)  # Explicitly move to cuda:0\n",
        "\n",
        "        # Generate response\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode and extract response\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        assistant_response = response.split(\"assistant<|end_header_id|>\")[-1].strip()\n",
        "        return assistant_response\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {str(e)}\"\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Summary and Memory Usage\n",
        "\n",
        "Display training summary and GPU memory usage.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Training summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total training samples: {len(train_dataset)}\")\n",
        "print(f\"Total validation samples: {len(eval_dataset)}\")\n",
        "print(f\"Training epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"Model output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Memory usage\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated(device='cuda:0') / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved(device='cuda:0') / 1024**3:.2f} GB\")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch Gradio Interface\n",
        "\n",
        "Launch a Gradio interface for interactive testing.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=chatbot_response,\n",
        "    inputs=gr.Textbox(label=\"Enter your question about EADE Business School\"),\n",
        "    outputs=gr.Textbox(label=\"Response\"),\n",
        "    title=\"EADE Business School Chatbot with RAG\",\n",
        "    description=\"Ask questions about EADE Business School, powered by a fine-tuned StableLM-2-Zephyr-1.6B with RAG.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "try:\n",
        "    interface.launch(share=True, debug=True)  # Note: share=True may not work in Kaggle; use local or deploy to Hugging Face Spaces\n",
        "    print(\"Gradio interface launched successfully. Access it via the public URL.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error launching Gradio interface: {str(e)}. In Kaggle, try running locally or deploy to Hugging Face Spaces.\")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}